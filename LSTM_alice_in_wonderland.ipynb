{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "LSTM_alice_in_wonderland.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristianCosci/LAB_MachineLearning_course/blob/main/LSTM_alice_in_wonderland.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FweUdCVwQh3S"
      },
      "source": [
        "# Problem\n",
        "Recurrent neural networks can also be used as **generative models**.\n",
        "\n",
        "This means that in addition to being used for predictive models (making predictions) \n",
        "they can learn the sequences of a problem and \n",
        "then generate entirely new plausible sequences for the problem domain.\n",
        "\n",
        "In this lesson we are going to use the dataset: ``Aliceâ€™s Adventures in Wonderland``.\n",
        "\n",
        "We are going to learn the dependencies between characters and \n",
        "the conditional probabilities of characters in sequences \n",
        "so that we can in turn generate wholly new and original sequences of characters."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHcsd2YJ2j9y",
        "outputId": "e216cf2c-223d-4c4c-b1ec-8064210ae409"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAyysZ74Qh3V"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import os"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKEbdmsEQh3h"
      },
      "source": [
        "### First step: Loading and marshalling of dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjoyvCSCQh3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad11573-1187-4511-8b29-f44495d49dec"
      },
      "source": [
        "### First step: Loading of dataset and marshaling of they# load ascii text and covert to lowercase\n",
        "### we need to load the ASCII text for the book into memory and convert all of the characters to \n",
        "### lowercase to reduce the vocabulary that the network must learn.\n",
        "filename = \"/content/drive/MyDrive/Colab Notebooks/wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()\n",
        "\n",
        "### We cannot model the characters directly, instead we must convert the characters to integers\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "\n",
        "\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = raw_text[i:i + seq_length]    \n",
        "    seq_out = raw_text[i + seq_length]\n",
        "  \n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(dataX[0])\n",
        "print(dataY[0])\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "\n",
        "\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "print(X.shape)\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "print(y.shape)\n",
        "print(y[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  144347\n",
            "Total Vocab:  45\n",
            "[17, 22, 15, 30, 34, 19, 32, 1, 23, 8, 1, 18, 29, 37, 28, 1, 34, 22, 19, 1, 32, 15, 16, 16, 23, 34, 7, 22, 29, 26, 19, 0, 0, 15, 26, 23, 17, 19, 1, 37, 15, 33, 1, 16, 19, 21, 23, 28, 28, 23, 28, 21, 1, 34, 29, 1, 21, 19, 34, 1, 36, 19, 32, 39, 1, 34, 23, 32, 19, 18, 1, 29, 20, 1, 33, 23, 34, 34, 23, 28, 21, 1, 16, 39, 1, 22, 19, 32, 1, 33, 23, 33, 34, 19, 32, 1, 29, 28, 1, 34]\n",
            "22\n",
            "Total Patterns:  144247\n",
            "(144247, 100, 1)\n",
            "(144247, 45)\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYRBqX9dSsO4"
      },
      "source": [
        "for idx,lista in enumerate(dataX):\n",
        "  if len(lista) !=100:\n",
        "    print(\"lists with diff lenght \",idx, len(lista))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkoH-HziQh3w"
      },
      "source": [
        "### Second step: Create the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd9P1exFQh3z"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.LSTM(256))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(y.shape[1], activation='softmax'))\n",
        "#filename = \"Alice_in_Wonderland_models/weights-improvement-05-1.9609-bigger.hdf5\"\n",
        "#model.load_weights(filename)  # note to self: it is also possible to save the model (save_model/load_model), \n",
        "                               # and then there is no need to defined the model before calling the load_weights()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KMxw8ctQh36"
      },
      "source": [
        "### Third step: define a callback which will save the intermedie weights of model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCYKlFxAQh38"
      },
      "source": [
        "# dirName = \"round_one\"\n",
        "# if not os.path.exists(dirName):\n",
        "#     os.mkdir(dirName)\n",
        "#     print(\"Directory \" , dirName ,  \" Created \")\n",
        "# else:    \n",
        "#     print(\"Directory \" , dirName ,  \" already exists\")\n",
        "filepath=\"round_one/weights-improvement-{epoch:02d}-{loss:.4f}-bigger2.hdf5\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hwI8RF3Qh4E"
      },
      "source": [
        "### Fourth step: Training of model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQDFwXJZQh4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0976080f-e8bd-4a25-a111-d905ac9afaa6"
      },
      "source": [
        "# fit the model\n",
        "model.fit(X, y, epochs=2, batch_size=64, callbacks=callbacks_list)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 2.8060\n",
            "Epoch 1: loss improved from inf to 2.80598, saving model to round_one/weights-improvement-01-2.8060-bigger2.hdf5\n",
            "2254/2254 [==============================] - 155s 65ms/step - loss: 2.8060\n",
            "Epoch 2/2\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 2.4015\n",
            "Epoch 2: loss improved from 2.80598 to 2.40152, saving model to round_one/weights-improvement-02-2.4015-bigger2.hdf5\n",
            "2254/2254 [==============================] - 145s 65ms/step - loss: 2.4015\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6ea06a1ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N-VEek1R6ST",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afb5b11f-51e7-41fb-a860-c7dc0ece53f0"
      },
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "print(\"start: \", start)\n",
        "pattern = dataX[start]\n",
        "print(\"pattern:\", pattern)\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start:  97817\n",
            "pattern: [34, 23, 29, 28, 19, 32, 1, 37, 19, 28, 34, 1, 29, 20, 20, 1, 26, 23, 25, 19, 1, 15, 28, 1, 15, 32, 32, 29, 37, 8, 0, 0, 1, 34, 22, 19, 1, 17, 15, 34, 42, 33, 1, 22, 19, 15, 18, 1, 16, 19, 21, 15, 28, 1, 20, 15, 18, 23, 28, 21, 1, 15, 37, 15, 39, 1, 34, 22, 19, 1, 27, 29, 27, 19, 28, 34, 1, 22, 19, 1, 37, 15, 33, 1, 21, 29, 28, 19, 6, 1, 15, 28, 18, 6, 0, 16, 39, 1, 34, 22]\n",
            "Seed:\n",
            "\" tioner went off like an arrow.\n",
            "\n",
            " the catâ€™s head began fading away the moment he was gone, and,\n",
            "by th \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR3eMDbeR8gs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7783d35-4b56-452e-b358-0dc3a1179cb5"
      },
      "source": [
        "# generate characters\n",
        "for i in range(100):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    #print(prediction)\n",
        "    index = np.argmax(prediction)\n",
        "    #print(index)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e mad ior a look a look a look a look a look a look a look a look a look a look a look a look a look\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaRbWQCtcGJL"
      },
      "source": [
        "filepath=\"round_two/weights-improvement-{epoch:02d}-{loss:.4f}-bigger2.hdf5\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChEE6DRscI4I"
      },
      "source": [
        "# fit the model\n",
        "model.fit(X, y, epochs=100, batch_size=64, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgyTNtv3WKnu"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model(\"/content/round_one/weights-improvement-60-1.1718-bigger2.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivReen_UcOWW"
      },
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "print(\"start: \", start)\n",
        "pattern = dataX[start]\n",
        "print(\"pattern:\", len(pattern))\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z3cB3A2cPzz"
      },
      "source": [
        "# generate characters\n",
        "for i in range(100):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    #print(\"x shape: \",x.shape)\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuqLBru7-999"
      },
      "source": [
        "X_train_nn = np.array(dataX)\n",
        "X_train_nn = X_train_nn/ float(n_vocab)\n",
        "print(X_train_nn.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EpP1Q7H_FyQ"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(256, input_shape=(100,), activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(256, activation= \"relu\"))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(y.shape[1], activation='softmax'))\n",
        "#filename = \"Alice_in_Wonderland_models/weights-improvement-05-1.9609-bigger.hdf5\"\n",
        "#model.load_weights(filename)  # note to self: it is also possible to save the model (save_model/load_model), \n",
        "                               # and then there is no need to defined the model before calling the load_weights()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.fit(X_train_nn, y, epochs=10, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEm7OpAH_S1h"
      },
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "print(\"start: \", start)\n",
        "pattern = dataX[start]\n",
        "print(\"pattern:\", len(pattern))\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZnJjIVp_VdL"
      },
      "source": [
        "# generate characters\n",
        "for i in range(100):\n",
        "    x = np.reshape(pattern, (1, len(pattern)))\n",
        "    #print(\"x shape: \",x.shape)\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}