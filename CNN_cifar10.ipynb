{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristianCosci/LAB_MachineLearning_course/blob/main/CNN_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeyGSNeno_O4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import np_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The CIFAR-10 dataset**\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n"
      ],
      "metadata": {
        "id": "QnjNX9W7yvNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()\n",
        "print(train_images.shape)\n",
        "print(train_images[0].shape)\n",
        "#print(train_images[0])\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "\n",
        "# Normalization\n",
        "train_images = train_images.astype('float32')\n",
        "test_images = test_images.astype('float32')\n",
        "train_images = train_images / 255.0\n",
        "test_images =  test_images / 255.0\n",
        "\n",
        "print(\"shape of train labels:\", train_labels.shape)\n",
        "print(\"some train labels\",train_labels[:10])"
      ],
      "metadata": {
        "id": "I9TMfnJdpYQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOtwALwZ45Qv"
      },
      "source": [
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    # The CIFAR labels happen to be arrays, \n",
        "    # which is why you need the extra index\n",
        "    plt.xlabel(class_names[train_labels[i][0]])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kwXaTah5ISE"
      },
      "source": [
        "# Create a CNN model  # add dropout and control overfitting # try to improve the accuracy\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(32, 32, 3))) \n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
        "# Need to pass from 2d data to 1d for the dense layer -> use a flatten layer\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(units=64, activation='relu'))\n",
        "model.add(layers.Dense(units=len(class_names), activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwNth86G5wsd"
      },
      "source": [
        "# Compile and train\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "model.save('model_cnn.h5')"
      ],
      "metadata": {
        "id": "17U6ZegrqmBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model evaluation**"
      ],
      "metadata": {
        "id": "UFb8iEqI0ke1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "id": "nnoaGE_Sqn3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion matrix**\n",
        "\n",
        "even if we use a different library to build our classifiers, we can still mix-and-match and use utilies functions from other libraries to analyze and evaluate the performance of a model. "
      ],
      "metadata": {
        "id": "pe-elbij0pz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = model.predict(test_images)\n",
        "matrix = confusion_matrix(test_labels, y_pred.argmax(axis=1))\n",
        "print(matrix)"
      ],
      "metadata": {
        "id": "t2J1b2k5rM3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sn\n",
        "import pandas  as pd \n",
        " \n",
        "df_cm = pd.DataFrame(matrix, range(10),range(10))\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.set(font_scale=1.4)#for label size\n",
        "sn.heatmap(df_cm, cmap=\"BuPu\",annot=True,annot_kws={\"size\": 10})# font size\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m5ssbeAVrPKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyze the outputs from intermediate convolutional layers**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4rLQeY5ZrYpP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SSgfV7a8GvO"
      },
      "source": [
        "model.layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2Tt6_uq8Qre"
      },
      "source": [
        "# Print the name and shape of the conv layers\n",
        "# Summarize feature map shapes\n",
        "for i in range(len(model.layers)):\n",
        "\tlayer = model.layers[i]\n",
        "\tif 'conv' not in layer.name:\n",
        "\t\tcontinue\n",
        "\t\n",
        "\tprint(i, layer.name, layer.output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new model using layers from the previous model\n",
        "# redefine model to output right after the first hidden layer\n",
        "model_v = keras.Model(inputs = model.inputs, outputs= model.layers[0].output)\n",
        "model_v.summary()"
      ],
      "metadata": {
        "id": "D_XKGfF9rtRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the feature maps for an images\n",
        "feature_maps = model_v.predict(train_images[4].reshape(1, 32, 32, 3))\n",
        "# print(feature_maps)\n",
        "print(feature_maps.shape)"
      ],
      "metadata": {
        "id": "6WRCWMnwrvXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(train_images[4])"
      ],
      "metadata": {
        "id": "b6jskiPkuktk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the feature maps\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "fig  = plt.figure(figsize=(12,12))\n",
        "\n",
        "for i in range(32):\n",
        "    sub = fig.add_subplot(8,4, i+1)\n",
        "    sub.imshow(feature_maps[0,:,:,i], cmap = \"gray\") \n",
        "plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])"
      ],
      "metadata": {
        "id": "NfCUOiRqrxp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repeat the above process using the second conv layer in our initial model\n",
        "# redefine model to output right after the second conv  layer\n",
        "model_v_2 = keras.Model(inputs = model.inputs, outputs= model.layers[2].output)\n",
        "model_v_2.summary()"
      ],
      "metadata": {
        "id": "Ww7TXfFbr2la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the feature maps for an images\n",
        "feature_maps_2 = model_v_2.predict(train_images[4].reshape(1, 32, 32, 3))\n",
        "#print(feature_maps_2)\n",
        "print(feature_maps_2.shape)"
      ],
      "metadata": {
        "id": "JvSsCInsr3-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the featue maps\n",
        "fig  = plt.figure(figsize=(12,12))\n",
        "\n",
        "for i in range(64):\n",
        "    sub = fig.add_subplot(8,8, i+1)\n",
        "    sub.imshow(feature_maps_2[0,:,:,i], cmap = \"gray\")\n",
        "plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])"
      ],
      "metadata": {
        "id": "_-OklKu-r6Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repeat the above process using the third conv layer in our initial model\n",
        "# redefine model to output right after the third conv  layer\n",
        "model_v_4 = keras.Model(inputs = model.inputs, outputs= model.layers[4].output)\n",
        "#TO DO: get the feature maps for an images\n",
        "feature_maps_4 = model_v_4.predict(train_images[4].reshape(1, 32, 32, 3))\n",
        "#print(feature_maps_2)\n",
        "print(feature_maps_4.shape)\n",
        "\n",
        "#TO DO : plot the featue maps\n",
        "fig  = plt.figure(figsize=(12,12))\n",
        "\n",
        "for i in range(64):\n",
        "    sub = fig.add_subplot(8,8, i+1)\n",
        "    sub.imshow(feature_maps_4[0,:,:,i], cmap = \"gray\")\n",
        "plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])"
      ],
      "metadata": {
        "id": "xSDbw6BLr9ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HOMEWORK:  plot the learned filters**\n",
        "\n",
        "Hint: the learned filters are simply the weights"
      ],
      "metadata": {
        "id": "d5XEWXIs4jmy"
      }
    }
  ]
}